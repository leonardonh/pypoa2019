{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping - Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o diretório raiz da aplicação\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Determinar opções do chrome para definir o diretório de download de arquivos\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\" : cwd}\n",
    "\n",
    "# Setar as opções (opções de tamanho da tela, posição do navegador, diretório para download, etc.)\n",
    "chromeOptions.add_experimental_option(\"prefs\",prefs)\n",
    "\n",
    "# Setar o diretório onde se encontra o chromedriver. Para utilizar o Selenium é necessário um driver para\n",
    "# fazer a interface com o navegador escolhido\n",
    "chromedriver = \"C:/Users/Leonardo/Desktop/palestra pypoa/chromedriver.exe\"\n",
    "\n",
    "# Instanciar o driver do Chrome com as opções definidas\n",
    "driver = webdriver.Chrome(executable_path=chromedriver, chrome_options=chromeOptions)\n",
    "\n",
    "# URL da página do repositório Lume da UFRGS\n",
    "driver.get(\"http://www.lume.ufrgs.br/handle/10183/15757\")\n",
    "\n",
    "\n",
    "enviar = driver.find_element_by_id('aspect_discovery_CommunityCollectionSearchFilter_field_submit-search')\n",
    "enviar.submit()\n",
    "trabalhos = np.arange(0,10)\n",
    "n_paginas = np.arange(1,2415)\n",
    "pagina = []\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Lista com a URL de todas as páginas\n",
    "for h in n_paginas:\n",
    "    pagina.append(\"https://lume.ufrgs.br/handle/10183/15757/discover?rpp=10&etal=0&group_by=none&page=\"+str(h)+\"&querytype_0=title&query_relational_operator_0=contains&query_value_0=&querytype_1=authortd&query_relational_operator_1=contains&query_value_1=&querytype_2=orientador&query_relational_operator_2=contains&query_value_2=&querytype_3=curso&query_relational_operator_3=equals&query_value_3=&querytype_4=subject&query_relational_operator_4=contains&query_value_4=&querytype_6=dataAno&query_relational_operator_6=equals&query_value_6=&querytype_7=idioma&query_relational_operator_7=equals&query_value_7=&querytype_8=formatoArquivo&query_relational_operator_8=equals&query_value_8=&query=\")\n",
    "\n",
    "# Loop de iteração por página\n",
    "for h in pagina:\n",
    "    driver.get(h)\n",
    "    div = driver.find_element_by_id('aspect_discovery_SimpleSearch_div_search-results')\n",
    "    \n",
    "    # Captura a URL dos 10 trabalhos contidos na página (links diplicados)\n",
    "    link =[]\n",
    "    for value in div.find_elements_by_css_selector(\"a\"): # os valores do elemento são acessados por iteração\n",
    "        link.append(value.get_attribute('href'))\n",
    "    \n",
    "    # Remove links duplicados\n",
    "    item = []\n",
    "    for value in np.arange(0,len(link),2):    \n",
    "        try:\n",
    "            item.append(link[value])\n",
    "        except:\n",
    "            pass\n",
    "    link = item\n",
    "    \n",
    "    # Loop de iteração para coletar informação de cada um dos 10 trabalhos contidos em cada página\n",
    "    for i in trabalhos:\n",
    "        \n",
    "        # Acessa a URL do trabalho\n",
    "        driver.get(link[i]+\"?show=full\")\n",
    "        \n",
    "        # Captura a informação da tabela que contém os dados de interesse\n",
    "        table = driver.find_element_by_class_name('ds-includeSet-table')\n",
    "        \n",
    "        # Captura a primeira coluna com o nome das variáveis\n",
    "        header =[]\n",
    "        for value in table.find_elements_by_class_name(\"label-cell\"):\n",
    "            header.append(value.text)\n",
    "        \n",
    "        # Captura os dados de toda a tabela\n",
    "        rows =[]\n",
    "        for value in table.find_elements_by_tag_name(\"td\"):\n",
    "            rows.append(value.text)\n",
    "        \n",
    "        # Remover o header dos dados que compõem as linhas\n",
    "        for value in header:\n",
    "            rows.remove(value)\n",
    "            \n",
    "        # Concatenar a coluna 2 com a coluna 3 da tabela e '@;' entre elas\n",
    "        item = []\n",
    "        for value in np.arange(0,len(rows),2):\n",
    "            try:\n",
    "                item.append(rows[value]+'@;'+rows[value+1])\n",
    "            except:\n",
    "                pass\n",
    "        rows = item\n",
    "        \n",
    "        # Tratamento especial para o resumo, pois pode ter mais de um. Todos são concatenados com '&;'\n",
    "        abstract = []\n",
    "        for i in range(len(header)):\n",
    "            if str(header[i]).find(\"abstract\") != -1:\n",
    "                abstract.append(i)         \n",
    "        a = \"\"\n",
    "        for i in (abstract):\n",
    "            a = rows[i] + \"&;\" + a\n",
    "            \n",
    "        # Tratamento especial para o autor, pois pode ter mais de um. Todos são concatenados com '&;'            \n",
    "        author = []\n",
    "        for i in range(len(header)):\n",
    "            if str(header[i]).find(\"author\") != -1:\n",
    "                author.append(i)             \n",
    "        b = \"\"\n",
    "        for i in (author):\n",
    "            b = rows[i] + \"&;\" + b\n",
    "         \n",
    "        # Criar o data frame no pandas\n",
    "        rows = pd.DataFrame(rows).T\n",
    "        rows.columns = header\n",
    "\n",
    "        # Deletar colunas duplicadas para cria-las novamente depois (evitar erro no append do pandas)\n",
    "        try:\n",
    "            rows = rows.drop(columns = [header[abstract[0]]])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            rows = rows.drop(columns = [header[author[0]]])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Informaçao da plataforma que será armazenada para cada trabalho\n",
    "        columns_keep = [\"dc.contributor.author\",\"dc.date.issued\",\"dc.identifier.uri\",\"dc.title\",\"dc.degree.graduation\",\"dc.description.abstract\",\"dc.degree.level\"]\n",
    "        rows['dc.description.abstract'] = a\n",
    "        rows['dc.contributor.author'] = b\n",
    "        rows = rows[columns_keep]\n",
    "\n",
    "        # Popular o o data frame com os dados coletados\n",
    "        if data.empty:\n",
    "            data = pd.DataFrame(columns = [\"dc.contributor.author\",\"dc.date.issued\",\"dc.identifier.uri\",\"dc.description.abstract\",\"dc.title\",\"dc.degree.graduation\",\"dc.degree.level\"])\n",
    "        data = data.append(rows,ignore_index=True)\n",
    "        \n",
    "        # Voltar para a página anterior e repetir o processo\n",
    "        driver.back()\n",
    "        \n",
    "# Salvar os dados coletados em .xlsx\n",
    "data.to_excel('dados_ufrgs_.xlsx', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
